{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ordered-partnership",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "enabling-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.nn.functional as F           # layers, activations and more\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fifth-preference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([1, 1])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-8390b996c350>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "layers = np.array([2, 50, 50, 50, 1]) #3 hidden layers\n",
    "\n",
    "W = []  #Weights and biases\n",
    "parameters = 0 #total number of parameters\n",
    "\n",
    "for i in range(len(layers)-1):\n",
    "\n",
    "    input_dim = layers[i]\n",
    "    output_dim = layers[i+1]\n",
    "\n",
    "    #Xavier standard deviation \n",
    "    std_dv = np.sqrt((2.0/(input_dim + output_dim)))\n",
    "\n",
    "    #weights = normal distribution * Xavier standard deviation + 0\n",
    "    w = torch.normal(0,1,size=(input_dim, output_dim),requires_grad=True) * std_dv\n",
    "\n",
    "    b = torch.zeros((output_dim,1),requires_grad=True)\n",
    "\n",
    "    print(b.shape)\n",
    "    \n",
    "    W.append(w)\n",
    "    W.append(b)\n",
    "\n",
    "    parameters +=  input_dim * output_dim + output_dim\n",
    "    \n",
    "N_u = 400 #Total number of data points for 'u'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "# Training data\n",
    "X_f_train, X_u_train, u_train = trainingdata(N_u,N_f)\n",
    "\n",
    "x = X_u_test\n",
    "x = torch.rand_like(torch.from_numpy(x), dtype=torch.float64)\n",
    "output = m(x)\n",
    "\n",
    "c = output.size\n",
    "\n",
    "print(list(c.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-platform",
   "metadata": {},
   "source": [
    "# *Data Prep*\n",
    "\n",
    "Training and Testing data is prepared from the solution file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "sensitive-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = np.linspace(-1,1,256)  # 256 points between -1 and 1 [256x1]\n",
    "x_2 = np.linspace(1,-1,256)  # 256 points between 1 and -1 [256x1]\n",
    "\n",
    "X, Y = np.meshgrid(x_1,x_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-psychiatry",
   "metadata": {},
   "source": [
    "# Test Data\n",
    "\n",
    "We prepare the test data to compare against the solution produced by the PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "reasonable-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u_test = np.hstack((X.flatten(order='F')[:,None], Y.flatten(order='F')[:,None]))\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array([-1, -1]) #lower bound\n",
    "ub = np.array([1, 1])  #upper bound\n",
    "\n",
    "a_1 = 1 \n",
    "a_2 = 1\n",
    "\n",
    "usol = np.sin(a_1 * np.pi * X) * np.sin(a_2 * np.pi * Y) #solution chosen for convinience  \n",
    "\n",
    "u = usol.flatten('F')[:,None] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-china",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "extended-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_u,N_f):\n",
    "    \n",
    "    leftedge_x = np.hstack((X[:,0][:,None], Y[:,0][:,None]))\n",
    "    leftedge_u = usol[:,0][:,None]\n",
    "    \n",
    "    rightedge_x = np.hstack((X[:,-1][:,None], Y[:,-1][:,None]))\n",
    "    rightedge_u = usol[:,-1][:,None]\n",
    "    \n",
    "    topedge_x = np.hstack((X[0,:][:,None], Y[0,:][:,None]))\n",
    "    topedge_u = usol[0,:][:,None]\n",
    "    \n",
    "    bottomedge_x = np.hstack((X[-1,:][:,None], Y[-1,:][:,None]))\n",
    "    bottomedge_u = usol[-1,:][:,None]\n",
    "    \n",
    "    all_X_u_train = np.vstack([leftedge_x, rightedge_x, bottomedge_x, topedge_x])\n",
    "    all_u_train = np.vstack([leftedge_u, rightedge_u, bottomedge_u, topedge_u])  \n",
    "     \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(all_X_u_train.shape[0], N_u, replace=False) \n",
    "    \n",
    "    X_u_train = all_X_u_train[idx[0:N_u], :] #choose indices from  set 'idx' (x,t)\n",
    "    u_train = all_u_train[idx[0:N_u],:]      #choose corresponding u\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    X_f = lb + (ub-lb)*lhs(2,N_f) \n",
    "    X_f_train = np.vstack((X_f, X_u_train)) # append training points to collocation points \n",
    "    \n",
    "    return X_f_train, X_u_train, u_train \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-nature",
   "metadata": {},
   "source": [
    "# Physics Informed Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "accepting-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel():\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "\n",
    "        self.W = []  #Weights and biases\n",
    "        self.parameters = 0 #total number of parameters\n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss = nn.MSELoss(reduction ='mean')\n",
    "\n",
    "        for i in range(len(layers)-1):\n",
    "            \n",
    "            input_dim = layers[i]\n",
    "            output_dim = layers[i+1]\n",
    "            \n",
    "            #Xavier standard deviation \n",
    "            std_dv = np.sqrt((2.0/(input_dim + output_dim)))\n",
    "            \n",
    "            #weights = normal distribution * Xavier standard deviation + 0\n",
    "            w = torch.normal(0,1,size=(input_dim, output_dim),requires_grad=True, dtype=torch.float64) * std_dv\n",
    "            \n",
    "            b = torch.zeros((output_dim),requires_grad=True)\n",
    "                       \n",
    "            self.W.append(w)\n",
    "            self.W.append(b)\n",
    "\n",
    "            self.parameters +=  input_dim * output_dim + output_dim\n",
    "            \n",
    "            \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = torch.rand_like(torch.from_numpy(x), dtype=torch.float64)\n",
    "                \n",
    "        #preprocessing input \n",
    "        x = (x - lb)/(ub - lb) #feature scaling\n",
    "        \n",
    "        a = x\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            \n",
    "            z = a.matmul(self.W[2*i]) + self.W[2*i+1]\n",
    "                        \n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = a.matmul(self.W[-2]) + self.W[-1]\n",
    "        \n",
    "        return a\n",
    "      \n",
    "#     def get_weights(self):\n",
    "\n",
    "#         parameters_1d = []  # [.... W_i,b_i.....  ] 1d array\n",
    "\n",
    "#         for i in range (len(layers)-1):\n",
    "            \n",
    "#             w_1d = W[2*i].view(-1)\n",
    "#             b_1d = W[2*i+1].view(-1)\n",
    "            \n",
    "#             parameters_1d = torch.cat([parameters_1d,w_1d],0)\n",
    "#             parameters_1d = torch.cat([parameters_1d,b_1d],0)\n",
    "        \n",
    "#         return parameters_1d   \n",
    "    \n",
    "    \n",
    "#     def set_weights(self,parameters):\n",
    "                \n",
    "#         for i in range (len(layers)-1):\n",
    "            \n",
    "#             shape_w = W[2*i].shape\n",
    "            \n",
    "#             tf.shape(self.W[2*i]).numpy() # shape of the weight tensor\n",
    "#             size_w = tf.size(self.W[2*i]).numpy() #size of the weight tensor \n",
    "            \n",
    "#             shape_b = tf.shape(self.W[2*i+1]).numpy() # shape of the bias tensor\n",
    "#             size_b = tf.size(self.W[2*i+1]).numpy() #size of the bias tensor \n",
    "            \n",
    "                        \n",
    "    def loss_BC(self,x,y):\n",
    "        \n",
    "        x = torch.rand_like(torch.from_numpy(x), dtype=torch.float64)\n",
    "        \n",
    "        y = torch.rand_like(torch.from_numpy(y), dtype=torch.float64)\n",
    "        \n",
    "        loss_u = self.loss(x, y)\n",
    "        \n",
    "        return loss_u\n",
    "    \n",
    "    def loss_PDE(self, x_to_train_f):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return loss_f, f\n",
    "    \n",
    "    def loss(self,x,y,g):\n",
    "\n",
    "        loss_u = self.loss_BC(x,y)\n",
    "        loss_f, f = self.loss_PDE(g)\n",
    "\n",
    "        loss = loss_u + loss_f\n",
    "\n",
    "        return loss, loss_u, loss_f \n",
    "     \n",
    "        \n",
    "#     def optimizerfunc(self,parameters):\n",
    "        \n",
    "#         return loss_val.numpy(), grads_1d.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "moving-exploration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1690, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "N_u = 400 #Total number of data points for 'u'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "# Training data\n",
    "X_f_train, X_u_train, u_train = trainingdata(N_u,N_f)\n",
    "\n",
    "layers = np.array([2, 50, 50, 50, 1]) #3 hidden layers\n",
    "\n",
    "maxcor = 200 \n",
    "max_iter = 5000\n",
    "\n",
    "PINN = Sequentialmodel(layers)\n",
    "\n",
    "u_pred = PINN.forward(X_u_test)\n",
    "\n",
    "l_bc = PINN.loss_BC(X_u_train, X_u_train)\n",
    "\n",
    "print(l_bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-parts",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_u = 400 #Total number of data points for 'u'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "# Training data\n",
    "X_f_train, X_u_train, u_train = trainingdata(N_u,N_f)\n",
    "\n",
    "layers = np.array([2, 50, 50, 50, 1]) #3 hidden layers\n",
    "\n",
    "maxcor = 200 \n",
    "max_iter = 5000\n",
    "\n",
    "PINN = Sequentialmodel(layers)\n",
    "\n",
    "init_params = PINN.get_weights().numpy()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.2f' % (elapsed))\n",
    "\n",
    "# optimization \n",
    "\n",
    "''' Model Accuracy ''' \n",
    "u_pred = PINN.evaluate(X_u_test)\n",
    "\n",
    "error_vec = np.linalg.norm((u-u_pred),2)/np.linalg.norm(u,2)        # Relative L2 Norm of the error (Vector)\n",
    "print('Test Error: %.5f'  % (error_vec))\n",
    "\n",
    "u_pred = np.reshape(u_pred,(256,256),order='F') \n",
    "\n",
    "\n",
    "# Plotting\n",
    "\n",
    "#Ground truth\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(x_1, x_2, usol, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$', fontsize=18)\n",
    "plt.ylabel(r'$x_2$', fontsize=18)\n",
    "plt.title('Ground Truth $u(x_1,x_2)$', fontsize=15)\n",
    "\n",
    "# Prediction\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(x_1, x_2, u_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$', fontsize=18)\n",
    "plt.ylabel(r'$x_2$', fontsize=18)\n",
    "plt.title('Predicted $\\hat u(x_1,x_2)$', fontsize=15)\n",
    "\n",
    "# Error\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(x_1, x_2, np.abs(usol - u_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$', fontsize=18)\n",
    "plt.ylabel(r'$x_2$', fontsize=18)\n",
    "plt.title(r'Absolute error $|u(x_1,x_2)- \\hat u(x_1,x_2)|$', fontsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "# fig.savefig('Helmholtz_non_stiff.png', dpi = 500, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
