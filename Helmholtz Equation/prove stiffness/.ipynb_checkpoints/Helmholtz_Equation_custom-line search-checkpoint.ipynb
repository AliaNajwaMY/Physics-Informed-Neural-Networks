{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "#hide tf logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'},\n",
    "#0 (default) shows all, 1 to filter out INFO logs, 2 to additionally filter out WARNING logs, and 3 to additionally filter out ERROR logs\n",
    "import scipy.optimize\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import codecs, json\n",
    "from scipy.optimize import minpack2\n",
    "\n",
    "# generates same random numbers each time\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Data Prep*\n",
    "\n",
    "Training and Testing data is prepared from the solution file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = np.linspace(-1,1,256)  # 256 points between -1 and 1 [256x1]\n",
    "x_2 = np.linspace(1,-1,256)  # 256 points between 1 and -1 [256x1]\n",
    "\n",
    "X, Y = np.meshgrid(x_1,x_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data\n",
    "\n",
    "We prepare the test data to compare against the solution produced by the PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u_test = np.hstack((X.flatten(order='F')[:,None], Y.flatten(order='F')[:,None]))\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array([-1, -1]) #lower bound\n",
    "ub = np.array([1, 1])  #upper bound\n",
    "\n",
    "a_1 = 1 \n",
    "a_2 = 4\n",
    "\n",
    "usol = np.sin(a_1 * np.pi * X) * np.sin(a_2 * np.pi * Y) #solution chosen for convinience  \n",
    "\n",
    "u = usol.flatten('F')[:,None] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_u,N_f):\n",
    "    \n",
    "    leftedge_x = np.hstack((X[:,0][:,None], Y[:,0][:,None]))\n",
    "    leftedge_u = usol[:,0][:,None]\n",
    "    \n",
    "    rightedge_x = np.hstack((X[:,-1][:,None], Y[:,-1][:,None]))\n",
    "    rightedge_u = usol[:,-1][:,None]\n",
    "    \n",
    "    topedge_x = np.hstack((X[0,:][:,None], Y[0,:][:,None]))\n",
    "    topedge_u = usol[0,:][:,None]\n",
    "    \n",
    "    bottomedge_x = np.hstack((X[-1,:][:,None], Y[-1,:][:,None]))\n",
    "    bottomedge_u = usol[-1,:][:,None]\n",
    "    \n",
    "    all_X_u_train = np.vstack([leftedge_x, rightedge_x, bottomedge_x, topedge_x])\n",
    "    all_u_train = np.vstack([leftedge_u, rightedge_u, bottomedge_u, topedge_u])  \n",
    "     \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(all_X_u_train.shape[0], N_u, replace=False) \n",
    "    \n",
    "    X_u_train = all_X_u_train[idx[0:N_u], :] #choose indices from  set 'idx' (x,t)\n",
    "    u_train = all_u_train[idx[0:N_u],:]      #choose corresponding u\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    X_f = lb + (ub-lb)*lhs(2,N_f) \n",
    "    X_f_train = np.vstack((X_f, X_u_train)) # append training points to collocation points \n",
    "    \n",
    "    return X_f_train, X_u_train, u_train \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN \n",
    "\n",
    "$W \\in \\mathcal{R}^{n_{l-1}\\times{n_l}}$ \n",
    "\n",
    "Creating sequential layers using the $\\textit{class}$ tf.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(tf.Module): \n",
    "    def __init__(self, layers, name=None):\n",
    "\n",
    "        self.W = []  #Weights and biases\n",
    "        self.parameters = 0 #total number of parameters\n",
    "\n",
    "        for i in range(len(layers)-1):\n",
    "\n",
    "            input_dim = layers[i]\n",
    "            output_dim = layers[i+1]\n",
    "\n",
    "            #Xavier standard deviation \n",
    "            std_dv = np.sqrt((2.0/(input_dim + output_dim)))\n",
    "\n",
    "            #weights = normal distribution * Xavier standard deviation + 0\n",
    "            w = tf.random.normal([input_dim, output_dim], dtype = 'float64') * std_dv\n",
    "\n",
    "            w = tf.Variable(w, trainable=True, name = 'w' + str(i+1))\n",
    "\n",
    "            b = tf.Variable(tf.cast(tf.zeros([output_dim]), dtype = 'float64'), trainable = True, name = 'b' + str(i+1))\n",
    "\n",
    "            self.W.append(w)\n",
    "            self.W.append(b)\n",
    "\n",
    "            self.parameters +=  input_dim * output_dim + output_dim\n",
    "            \n",
    "        self.X = np.zeros(self.parameters) #store iterates\n",
    "        self.G = np.zeros(self.parameters) #store gradients\n",
    "        self.store = np.zeros((max_iter,2)) #store computed values for plotting\n",
    "        self.iter_counter = 0 # iteration counter for optimizer\n",
    "    \n",
    "    def evaluate(self,x):\n",
    "        \n",
    "        #preprocessing input \n",
    "        x = (x - lb)/(ub - lb) #feature scaling\n",
    "        \n",
    "        a = x\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            \n",
    "            z = tf.add(tf.matmul(a, self.W[2*i]), self.W[2*i+1])\n",
    "            a = tf.nn.tanh(z)\n",
    "            \n",
    "        a = tf.add(tf.matmul(a, self.W[-2]), self.W[-1]) # For regression, no activation to last layer\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def get_weights(self):\n",
    "\n",
    "        parameters_1d = []  # [.... W_i,b_i.....  ] 1d array\n",
    "        \n",
    "        for i in range (len(layers)-1):\n",
    "            \n",
    "            w_1d = tf.reshape(self.W[2*i],[-1])   #flatten weights \n",
    "            b_1d = tf.reshape(self.W[2*i+1],[-1]) #flatten biases\n",
    "            \n",
    "            parameters_1d = tf.concat([parameters_1d, w_1d], 0) #concat weights \n",
    "            parameters_1d = tf.concat([parameters_1d, b_1d], 0) #concat biases\n",
    "        \n",
    "        return parameters_1d\n",
    "        \n",
    "    def set_weights(self,parameters):\n",
    "                \n",
    "        for i in range (len(layers)-1):\n",
    "\n",
    "            shape_w = tf.shape(self.W[2*i]).numpy() # shape of the weight tensor\n",
    "            size_w = tf.size(self.W[2*i]).numpy() #size of the weight tensor \n",
    "            \n",
    "            shape_b = tf.shape(self.W[2*i+1]).numpy() # shape of the bias tensor\n",
    "            size_b = tf.size(self.W[2*i+1]).numpy() #size of the bias tensor \n",
    "                        \n",
    "            pick_w = parameters[0:size_w] #pick the weights \n",
    "            self.W[2*i].assign(tf.reshape(pick_w,shape_w)) # assign  \n",
    "            parameters = np.delete(parameters,np.arange(size_w),0) #delete \n",
    "            \n",
    "            pick_b = parameters[0:size_b] #pick the biases \n",
    "            self.W[2*i+1].assign(tf.reshape(pick_b,shape_b)) # assign \n",
    "            parameters = np.delete(parameters,np.arange(size_b),0) #delete \n",
    "\n",
    "            \n",
    "    def loss_BC(self,x,y):\n",
    "\n",
    "        loss_u = tf.reduce_mean(tf.square(y-self.evaluate(x)))\n",
    "        return loss_u\n",
    "\n",
    "    def loss_PDE(self, x_to_train_f):\n",
    "    \n",
    "        g = tf.Variable(x_to_train_f, dtype = 'float64', trainable = False)\n",
    "\n",
    "        k = 1    \n",
    "\n",
    "        x_1_f = g[:,0:1]\n",
    "        x_2_f = g[:,1:2]\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            tape.watch(x_1_f)\n",
    "            tape.watch(x_2_f)\n",
    "\n",
    "            g = tf.stack([x_1_f[:,0], x_2_f[:,0]], axis=1)\n",
    "\n",
    "            u = self.evaluate(g)\n",
    "            u_x_1 = tape.gradient(u,x_1_f)\n",
    "            u_x_2 = tape.gradient(u,x_2_f)\n",
    "\n",
    "        u_xx_1 = tape.gradient(u_x_1,x_1_f)\n",
    "        u_xx_2 = tape.gradient(u_x_2,x_2_f)\n",
    "\n",
    "        del tape\n",
    "\n",
    "        q = -( (a_1*np.pi)**2 + (a_2*np.pi)**2 - k**2 ) * np.sin(a_1*np.pi*x_1_f) * np.sin(a_2*np.pi*x_2_f)\n",
    "\n",
    "        f = u_xx_1 + u_xx_2 + k**2 * u - q #residual\n",
    "\n",
    "        loss_f = tf.reduce_mean(tf.square(f))\n",
    "\n",
    "        return loss_f, f\n",
    "    \n",
    "    def loss(self,x,y,g):\n",
    "\n",
    "        loss_u = self.loss_BC(x,y)\n",
    "        loss_f, f = self.loss_PDE(g)\n",
    "\n",
    "        loss = loss_u + loss_f\n",
    "\n",
    "        return loss, loss_u, loss_f \n",
    "    \n",
    "    def optimizerfunc(self,parameters):\n",
    "        \n",
    "        self.set_weights(parameters)\n",
    "       \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.trainable_variables)\n",
    "            \n",
    "            loss_val, loss_u, loss_f = self.loss(X_u_train, u_train, X_f_train)\n",
    "            \n",
    "        grads = tape.gradient(loss_val,self.trainable_variables)\n",
    "                \n",
    "        del tape\n",
    "        \n",
    "        grads_1d = [ ] #store 1d grads \n",
    "        \n",
    "        for i in range (len(layers)-1):\n",
    "\n",
    "            grads_w_1d = tf.reshape(grads[2*i],[-1]) #flatten weights \n",
    "            grads_b_1d = tf.reshape(grads[2*i+1],[-1]) #flatten biases\n",
    "\n",
    "            grads_1d = tf.concat([grads_1d, grads_w_1d], 0) #concat grad_weights \n",
    "            grads_1d = tf.concat([grads_1d, grads_b_1d], 0) #concat grad_biases\n",
    "        \n",
    "        return loss_val.numpy(), grads_1d.numpy()\n",
    "    \n",
    "    def optimizer_callback(self,parameters):\n",
    "                \n",
    "        loss_value, loss_u, loss_f = self.loss(X_u_train, u_train, X_f_train)\n",
    "        \n",
    "        u_pred = self.evaluate(X_u_test)\n",
    "        error_vec = np.linalg.norm((u-u_pred),2)/np.linalg.norm(u,2)\n",
    "        \n",
    "        alpha, f_newval = self.LbfgsInvHessProduct(parameters)\n",
    "        \n",
    "        tf.print(loss_value, f_newval, loss_value-f_newval, alpha)\n",
    "        \n",
    "    def LbfgsInvHessProduct(self,parameters):\n",
    "\n",
    "        self.iter_counter += 1  #update iteration counter \n",
    "\n",
    "        x_k = parameters  \n",
    "\n",
    "        self.X = np.vstack((x_k.T,self.X)) #stack latest value on top row\n",
    "\n",
    "        old_fval,g_k = self.optimizerfunc(parameters) #obtain grads and loss value\n",
    "\n",
    "        self.G = np.vstack((g_k.T,self.G)) #stack latest grads on top row\n",
    "\n",
    "        n_corrs = min(self.iter_counter, maxcor) #for iterations < maxcor, we will take all available updates\n",
    "        \n",
    "        sk = self.X = self.X[:n_corrs] #select top 'n_corrs' x values, with latest value on top by construction\n",
    "        yk = self.G = self.G[:n_corrs] #select top 'n_corrs' gradient values, with latest value on top by construction \n",
    "\n",
    "        #linear operator B_k_inv    \n",
    "        hess_inv = scipy.optimize.LbfgsInvHessProduct(sk,yk) #instantiate class\n",
    "\n",
    "        p_k = - hess_inv.matvec(g_k) #p_k = -B_k_inv * g_k\n",
    "\n",
    "        gkpk = np.dot(p_k,g_k) #term 1 in report\n",
    "\n",
    "        norm_p_k_sq = (np.linalg.norm(p_k,ord=2))**2 # norm squared\n",
    "               \n",
    "        #store the values\n",
    "        self.store[self.iter_counter-1] = [gkpk,norm_p_k_sq]\n",
    "        \n",
    "        def line_search_armijo(f, xk, pk, gfk, old_fval, args=(), c1=1e-4, alpha0=1):\n",
    "            \"\"\"Minimize over alpha, the function ``f(xk+alpha pk)``.\n",
    "            Parameters\n",
    "            ----------\n",
    "            f : callable\n",
    "                Function to be minimized.\n",
    "            xk : array_like\n",
    "                Current point.\n",
    "            pk : array_like\n",
    "                Search direction.\n",
    "            gfk : array_like\n",
    "                Gradient of `f` at point `xk`.\n",
    "            old_fval : float\n",
    "                Value of `f` at point `xk`.\n",
    "            args : tuple, optional\n",
    "                Optional arguments.\n",
    "            c1 : float, optional\n",
    "                Value to control stopping criterion.\n",
    "            alpha0 : scalar, optional\n",
    "                Value of `alpha` at start of the optimization.\n",
    "            Returns\n",
    "            -------\n",
    "            alpha\n",
    "            f_count\n",
    "            f_val_at_alpha\n",
    "            Notes\n",
    "            -----\n",
    "            Uses the interpolation algorithm (Armijo backtracking) as suggested by\n",
    "            Wright and Nocedal in 'Numerical Optimization', 1999, pp. 56-57\n",
    "            \"\"\"\n",
    "            xk = np.atleast_1d(xk)\n",
    "            fc = [0]\n",
    "\n",
    "            def phi(alpha1):\n",
    "                fc[0] += 1\n",
    "                return f(xk + alpha1*pk, *args)\n",
    "\n",
    "            if old_fval is None:\n",
    "                phi0 = phi(0.)\n",
    "            else:\n",
    "                phi0 = old_fval  # compute f(xk) -- done in past loop\n",
    "\n",
    "            derphi0 = np.dot(gfk, pk)\n",
    "            alpha, phi1 = scalar_search_armijo(phi, phi0, derphi0, c1=c1,\n",
    "                                               alpha0=alpha0)\n",
    "            return alpha, fc[0], phi1\n",
    "\n",
    "\n",
    "        def line_search_BFGS(f, xk, pk, gfk, old_fval, args=(), c1=1e-4, alpha0=1):\n",
    "            \"\"\"\n",
    "            Compatibility wrapper for `line_search_armijo`\n",
    "            \"\"\"\n",
    "            r = line_search_armijo(f, xk, pk, gfk, old_fval, args=args, c1=c1,\n",
    "                                   alpha0=alpha0)\n",
    "            return r[0], r[1], 0, r[2]\n",
    "\n",
    "\n",
    "        def scalar_search_armijo(phi, phi0, derphi0, c1=1e-4, alpha0=1, amin=0):\n",
    "            \"\"\"Minimize over alpha, the function ``phi(alpha)``.\n",
    "            Uses the interpolation algorithm (Armijo backtracking) as suggested by\n",
    "            Wright and Nocedal in 'Numerical Optimization', 1999, pp. 56-57\n",
    "            alpha > 0 is assumed to be a descent direction.\n",
    "            Returns\n",
    "            -------\n",
    "            alpha\n",
    "            phi1\n",
    "            \"\"\"\n",
    "            phi_a0 = phi(alpha0)\n",
    "            if (phi_a0 <=  phi0 + c1*alpha0*derphi0):\n",
    "                return alpha0, phi_a0\n",
    "\n",
    "            # Otherwise, compute the minimizer of a quadratic interpolant:\n",
    "\n",
    "            alpha1 = -(derphi0) * alpha0**2 / 2.0 / (phi_a0 - phi0 - derphi0 * alpha0)\n",
    "            phi_a1 = phi(alpha1)\n",
    "\n",
    "            if (phi_a1 <= phi0 + c1*alpha1*derphi0):\n",
    "                return alpha1, phi_a1\n",
    "\n",
    "            # Otherwise, loop with cubic interpolation until we find an alpha which\n",
    "            # satisfies the first Wolfe condition (since we are backtracking, we will\n",
    "            # assume that the value of alpha is not too small and satisfies the second\n",
    "            # condition.\n",
    "\n",
    "            while alpha1 > amin:       # we are assuming alpha>0 is a descent direction\n",
    "                factor = alpha0**2 * alpha1**2 * (alpha1-alpha0)\n",
    "                a = alpha0**2 * (phi_a1 - phi0 - derphi0*alpha1) - \\\n",
    "                    alpha1**2 * (phi_a0 - phi0 - derphi0*alpha0)\n",
    "                a = a / factor\n",
    "                b = -alpha0**3 * (phi_a1 - phi0 - derphi0*alpha1) + \\\n",
    "                    alpha1**3 * (phi_a0 - phi0 - derphi0*alpha0)\n",
    "                b = b / factor\n",
    "\n",
    "                alpha2 = (-b + np.sqrt(abs(b**2 - 3 * a * derphi0))) / (3.0*a)\n",
    "                phi_a2 = phi(alpha2)\n",
    "\n",
    "                if (phi_a2 <= phi0 + c1*alpha2*derphi0):\n",
    "                    return alpha2, phi_a2\n",
    "\n",
    "                if (alpha1 - alpha2) > alpha1 / 2.0 or (1 - alpha2/alpha1) < 0.96:\n",
    "                    alpha2 = alpha1 / 2.0\n",
    "\n",
    "                alpha0 = alpha1\n",
    "                alpha1 = alpha2\n",
    "                phi_a0 = phi_a1\n",
    "                phi_a1 = phi_a2\n",
    "\n",
    "            # Failed to find a suitable step length\n",
    "            return None, phi_a1\n",
    "        \n",
    "        def ls_function(x):\n",
    "            val, _ = self.optimizerfunc(x)\n",
    "            return val\n",
    "        \n",
    "        alpha, _, _, f_newval = line_search_BFGS(ls_function, x_k, p_k, g_k, old_fval, args=(), c1=1e-4, alpha0=1)    \n",
    "        \n",
    "        return alpha, f_newval\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Loss Function*\n",
    "\n",
    "The loss function consists of two parts:\n",
    "1. **loss_BC**: MSE error of boundary losses\n",
    "2. **loss_PDE**: MSE error of collocation points satisfying the PDE\n",
    "\n",
    "**loss** = loss_BC + loss_PDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "84.518615140963334 84.51860409376859 1.104719474653848e-05 -0.0003633599705001406\n",
      "84.516940601758861 84.51694059982557 1.9332873080202262e-09 -6.861359537960459e-08\n",
      "84.504280733142167 84.50425952630324 2.1206838923149007e-05 5.939705230099322e-05\n",
      "84.49745397934096 84.49745274618297 1.233157988167477e-06 2.047540826070801e-06\n",
      "Training time: 12.54\n",
      "      fun: 84.49745397934096\n",
      " hess_inv: <5301x5301 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([ 0.00432124, -0.00888981,  0.00670883, ..., -0.01213488,\n",
      "       -0.00457839, -0.08033026])\n",
      "  message: b'STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT'\n",
      "     nfev: 6\n",
      "      nit: 4\n",
      "     njev: 6\n",
      "   status: 1\n",
      "  success: False\n",
      "        x: array([ 0.13373143,  0.01946636, -0.09126716, ...,  0.11480737,\n",
      "        0.24000531, -0.0376938 ])\n",
      "Test Error: 1.02613\n"
     ]
    }
   ],
   "source": [
    "N_u = 400 #Total number of data points for 'u'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "# Training data\n",
    "X_f_train, X_u_train, u_train = trainingdata(N_u,N_f)\n",
    "\n",
    "layers = np.array([2, 50, 50, 50, 1]) #3 hidden layers\n",
    "\n",
    "maxcor = 200 \n",
    "max_iter = 5000\n",
    "\n",
    "PINN = Sequentialmodel(layers)\n",
    "\n",
    "init_params = PINN.get_weights().numpy()\n",
    "\n",
    "start_time = time.time() \n",
    "\n",
    "# train the model with Scipy L-BFGS optimizer\n",
    "results = scipy.optimize.minimize(fun = PINN.optimizerfunc, \n",
    "                                  x0 = init_params, \n",
    "                                  args=(), \n",
    "                                  method='L-BFGS-B', \n",
    "                                  jac= True,        # If jac is True, fun is assumed to return the gradient along with the objective function\n",
    "                                  callback = PINN.optimizer_callback, \n",
    "                                  options = {'disp': None,\n",
    "                                            'maxcor': maxcor, \n",
    "                                            'ftol': 1 * np.finfo(float).eps,  #The iteration stops when (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol\n",
    "                                            'gtol': 5e-10, \n",
    "                                            'maxfun':  max_iter*1.5, \n",
    "                                            'maxiter': max_iter,\n",
    "                                            'iprint': -1,   #print update every 50 iterations\n",
    "                                            'maxls': 50})\n",
    "\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.2f' % (elapsed))\n",
    "\n",
    "print(results)\n",
    "\n",
    "# np.savetxt('values_stiff.txt', PINN.store)\n",
    "\n",
    "PINN.set_weights(results.x)\n",
    "\n",
    "''' Model Accuracy ''' \n",
    "u_pred = PINN.evaluate(X_u_test)\n",
    "\n",
    "error_vec = np.linalg.norm((u-u_pred),2)/np.linalg.norm(u,2)        # Relative L2 Norm of the error (Vector)\n",
    "print('Test Error: %.5f'  % (error_vec))\n",
    "\n",
    "u_pred = np.reshape(u_pred,(256,256),order='F') \n",
    "\n",
    "# #Plot and save image\n",
    "# plt.pcolor(x_1,x_2,u_pred, cmap = 'jet')\n",
    "# plt.axis('scaled')\n",
    "# plt.colorbar()\n",
    "# plt.savefig('Stiff_Helmholtz.png', dpi = 500)\n",
    "\n",
    "# #Error plot\n",
    "# plt.pcolor(x_1,x_2,np.abs(usol-u_pred), cmap = 'jet')\n",
    "# plt.axis('scaled')\n",
    "# plt.colorbar()\n",
    "# plt.savefig('Stiff_Helmholtz_error.png', dpi = 500)\n",
    "# plt.close()\n",
    "\n",
    "# #Residual plot\n",
    "# loss_f, f_plot = PINN.loss_PDE(X_u_test)\n",
    "# plt.scatter(X_u_test[:,0:1], X_u_test[:,1:2], c=f_plot, cmap = 'jet')\n",
    "# plt.axis('scaled')\n",
    "# plt.colorbar()\n",
    "# plt.savefig('Stiff_Helmholtz_residual.png', dpi = 500)\n",
    "\n",
    "# #plot gkpk\n",
    "# plt.semilogy(PINN.store[:,0])\n",
    "# plt.yscale('symlog')\n",
    "# plt.savefig('gkpk_stiff.png', dpi = 500)\n",
    "\n",
    "# #plot norm_p_k_sq\n",
    "# plt.semilogy(PINN.store[:,1])\n",
    "# plt.yscale('symlog')\n",
    "# plt.savefig('norm_p_k_sq_stiff.png', dpi = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       difference         loss         alpha  loss_predict\n",
      "0     6647.977180  6647.970748  6.432492e-03  4.681621e-01\n",
      "1     6647.887445  6647.887376  6.847833e-05  8.636860e-05\n",
      "2     6647.668496  6647.668488  8.174075e-06  1.286543e-06\n",
      "3     6647.605452  6647.603448  2.003287e-03  7.581114e-03\n",
      "4     6647.588016  6647.543211  4.480489e-02  4.031692e-01\n",
      "...           ...          ...           ...           ...\n",
      "4995     0.024115     0.024115  3.469447e-18  5.023474e-20\n",
      "4996     0.024112     0.024112  5.780931e-13  1.509580e-13\n",
      "4997     0.024109     0.024109  5.801366e-13  1.449693e-13\n",
      "4998     0.024107     0.024107  5.621491e-08  2.059545e-07\n",
      "4999     0.024103     0.024103  2.433602e-10  1.972284e-10\n",
      "\n",
      "[5000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "s1 = np.loadtxt('non_stiff.txt', comments = \"#\", delimiter = \" \", unpack = False)\n",
    "s2 = np.loadtxt('stiff.txt', comments = \"#\", delimiter = \" \", unpack = False)\n",
    "\n",
    "df1 = pd.DataFrame(data = s1, columns = {\"loss\", \"loss_predict\", \"difference\", \"alpha\"})\n",
    "df2 = pd.DataFrame(data = s2, columns = {\"loss\", \"loss_predict\", \"difference\", \"alpha\"})\n",
    "\n",
    "\n",
    "# df1.replace(to_replace=7259365998, value=np.nan)\n",
    "# df2.replace(to_replace=7259365998, value=np.nan)\n",
    "\n",
    "\n",
    "# #plot difference\n",
    "# fig,ax = plt.subplots()\n",
    "# plt.semilogy(s1[:,2], label = 'Non-stiff')\n",
    "# plt.semilogy(s2[:,2], label = 'Stiff')\n",
    "# plt.yscale('symlog')\n",
    "# plt.xlabel('iterations')\n",
    "# plt.ylabel('$difference$')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-78e1b55d41c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nan' is not defined"
     ]
    }
   ],
   "source": [
    "print(np.nan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
